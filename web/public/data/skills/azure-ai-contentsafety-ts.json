{
  "id": "antigravity-azure-ai-contentsafety-ts",
  "name": "azure-ai-contentsafety-ts",
  "slug": "azure-ai-contentsafety-ts",
  "description": "Analyze text and images for harmful content using Azure AI Content Safety (@azure-rest/ai-content-safety). Use when moderating user-generated content, detecting hate speech, violence, sexual content, or self-harm, or managing custom blocklists.",
  "category": "AI & Agents",
  "source": "antigravity",
  "repoUrl": "https://github.com/sickn33/antigravity-awesome-skills",
  "skillUrl": "https://github.com/sickn33/antigravity-awesome-skills/tree/main/skills/azure-ai-contentsafety-ts",
  "content": "\n# Azure AI Content Safety REST SDK for TypeScript\n\nAnalyze text and images for harmful content with customizable blocklists.\n\n## Installation\n\n```bash\nnpm install @azure-rest/ai-content-safety @azure/identity @azure/core-auth\n```\n\n## Environment Variables\n\n```bash\nCONTENT_SAFETY_ENDPOINT=https://<resource>.cognitiveservices.azure.com\nCONTENT_SAFETY_KEY=<api-key>\n```\n\n## Authentication\n\n**Important**: This is a REST client. `ContentSafetyClient` is a **function**, not a class.\n\n### API Key\n\n```typescript\nimport ContentSafetyClient from \"@azure-rest/ai-content-safety\";\nimport { AzureKeyCredential } from \"@azure/core-auth\";\n\nconst client = ContentSafetyClient(\n  process.env.CONTENT_SAFETY_ENDPOINT!,\n  new AzureKeyCredential(process.env.CONTENT_SAFETY_KEY!)\n);\n```\n\n### DefaultAzureCredential\n\n```typescript\nimport ContentSafetyClient from \"@azure-rest/ai-content-safety\";\nimport { DefaultAzureCredential } from \"@azure/identity\";\n\nconst client = ContentSafetyClient(\n  process.env.CONTENT_SAFETY_ENDPOINT!,\n  new DefaultAzureCredential()\n);\n```\n\n## Analyze Text\n\n```typescript\nimport ContentSafetyClient, { isUnexpected } from \"@azure-rest/ai-content-safety\";\n\nconst result = await client.path(\"/text:analyze\").post({\n  body: {\n    text: \"Text content to analyze\",\n    categories: [\"Hate\", \"Sexual\", \"Violence\", \"SelfHarm\"],\n    outputType: \"FourSeverityLevels\"  // or \"EightSeverityLevels\"\n  }\n});\n\nif (isUnexpected(result)) {\n  throw result.body;\n}\n\nfor (const analysis of result.body.categoriesAnalysis) {\n  console.log(`${analysis.category}: severity ${analysis.severity}`);\n}\n```\n\n## Analyze Image\n\n### Base64 Content\n\n```typescript\nimport { readFileSync } from \"node:fs\";\n\nconst imageBuffer = readFileSync(\"./image.png\");\nconst base64Image = imageBuffer.toString(\"base64\");\n\nconst result = await client.path(\"/image:analyze\").post({\n  body: {\n    image: { content: base64Image }\n  }\n});\n\nif (isUnexpected(result)) {\n  throw result.body;\n}\n\nfor (const analysis of result.body.categoriesAnalysis) {\n  console.log(`${analysis.category}: severity ${analysis.severity}`);\n}\n```\n\n### Blob URL\n\n```typescript\nconst result = await client.path(\"/image:analyze\").post({\n  body: {\n    image: { blobUrl: \"https://storage.blob.core.windows.net/container/image.png\" }\n  }\n});\n```\n\n## Blocklist Management\n\n### Create Blocklist\n\n```typescript\nconst result = await client\n  .path(\"/text/blocklists/{blocklistName}\", \"my-blocklist\")\n  .patch({\n    contentType: \"application/merge-patch+json\",\n    body: {\n      description: \"Custom blocklist for prohibited terms\"\n    }\n  });\n\nif (isUnexpected(result)) {\n  throw result.body;\n}\n\nconsole.log(`Created: ${result.body.blocklistName}`);\n```\n\n### Add Items to Blocklist\n\n```typescript\nconst result = await client\n  .path(\"/text/blocklists/{blocklistName}:addOrUpdateBlocklistItems\", \"my-blocklist\")\n  .post({\n    body: {\n      blocklistItems: [\n        { text: \"prohibited-term-1\", description: \"First blocked term\" },\n        { text: \"prohibited-term-2\", description: \"Second blocked term\" }\n      ]\n    }\n  });\n\nif (isUnexpected(result)) {\n  throw result.body;\n}\n\nfor (const item of result.body.blocklistItems ?? []) {\n  console.log(`Added: ${item.blocklistItemId}`);\n}\n```\n\n### Analyze with Blocklist\n\n```typescript\nconst result = await client.path(\"/text:analyze\").post({\n  body: {\n    text: \"Text that might contain blocked terms\",\n    blocklistNames: [\"my-blocklist\"],\n    haltOnBlocklistHit: false\n  }\n});\n\nif (isUnexpected(result)) {\n  throw result.body;\n}\n\n// Check blocklist matches\nif (result.body.blocklistsMatch) {\n  for (const match of result.body.blocklistsMatch) {\n    console.log(`Blocked: \"${match.blocklistItemText}\" from ${match.blocklistName}`);\n  }\n}\n```\n\n### List Blocklists\n\n```typescript\nconst result = await client.path(\"/text/blocklists\").get();\n\nif (isUnexpected(result)) {\n  throw result.body;\n}\n\nfor (const blocklist of result.body.value ?? []) {\n  console.log(`${blocklist.blocklistName}: ${blocklist.description}`);\n}\n```\n\n### Delete Blocklist\n\n```typescript\nawait client.path(\"/text/blocklists/{blocklistName}\", \"my-blocklist\").delete();\n```\n\n## Harm Categories\n\n| Category | API Term | Description |\n|----------|----------|-------------|\n| Hate and Fairness | `Hate` | Discriminatory language targeting identity groups |\n| Sexual | `Sexual` | Sexual content, nudity, pornography |\n| Violence | `Violence` | Physical harm, weapons, terrorism |\n| Self-Harm | `SelfHarm` | Self-injury, suicide, eating disorders |\n\n## Severity Levels\n\n| Level | Risk | Recommended Action |\n|-------|------|-------------------|\n| 0 | Safe | Allow |\n| 2 | Low | Review or allow with warning |\n| 4 | Medium | Block or require human review |\n| 6 | High | Block immediately |\n\n**Output Types**:\n- `FourSeverityLevels` (default): Returns 0, 2, 4, 6\n- `EightSeverityLevels`: Returns 0-7\n\n## Content Moderation Helper\n\n```typescript\nimport ContentSafetyClient, { \n  isUnexpected, \n  TextCategoriesAnalysisOutput \n} from \"@azure-rest/ai-content-safety\";",
  "tags": [
    "typescript",
    "node",
    "api",
    "ai",
    "image",
    "azure",
    "rag"
  ],
  "useCases": [],
  "scrapedAt": "2026-02-15T07:03:20.626Z"
}
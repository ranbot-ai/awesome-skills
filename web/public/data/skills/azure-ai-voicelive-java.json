{
  "id": "antigravity-azure-ai-voicelive-java",
  "name": "azure-ai-voicelive-java",
  "slug": "azure-ai-voicelive-java",
  "description": "Azure AI VoiceLive SDK for Java. Real-time bidirectional voice conversations with AI assistants using WebSocket.\nTriggers: \"VoiceLiveClient java\", \"voice assistant java\", \"real-time voice java\", \"audio streaming java\", \"voice activity detection java\".\n",
  "category": "AI & Agents",
  "source": "antigravity",
  "repoUrl": "https://github.com/sickn33/antigravity-awesome-skills",
  "skillUrl": "https://github.com/sickn33/antigravity-awesome-skills/tree/main/skills/azure-ai-voicelive-java",
  "content": "\n# Azure AI VoiceLive SDK for Java\n\nReal-time, bidirectional voice conversations with AI assistants using WebSocket technology.\n\n## Installation\n\n```xml\n<dependency>\n    <groupId>com.azure</groupId>\n    <artifactId>azure-ai-voicelive</artifactId>\n    <version>1.0.0-beta.2</version>\n</dependency>\n```\n\n## Environment Variables\n\n```bash\nAZURE_VOICELIVE_ENDPOINT=https://<resource>.openai.azure.com/\nAZURE_VOICELIVE_API_KEY=<your-api-key>\n```\n\n## Authentication\n\n### API Key\n\n```java\nimport com.azure.ai.voicelive.VoiceLiveAsyncClient;\nimport com.azure.ai.voicelive.VoiceLiveClientBuilder;\nimport com.azure.core.credential.AzureKeyCredential;\n\nVoiceLiveAsyncClient client = new VoiceLiveClientBuilder()\n    .endpoint(System.getenv(\"AZURE_VOICELIVE_ENDPOINT\"))\n    .credential(new AzureKeyCredential(System.getenv(\"AZURE_VOICELIVE_API_KEY\")))\n    .buildAsyncClient();\n```\n\n### DefaultAzureCredential (Recommended)\n\n```java\nimport com.azure.identity.DefaultAzureCredentialBuilder;\n\nVoiceLiveAsyncClient client = new VoiceLiveClientBuilder()\n    .endpoint(System.getenv(\"AZURE_VOICELIVE_ENDPOINT\"))\n    .credential(new DefaultAzureCredentialBuilder().build())\n    .buildAsyncClient();\n```\n\n## Key Concepts\n\n| Concept | Description |\n|---------|-------------|\n| `VoiceLiveAsyncClient` | Main entry point for voice sessions |\n| `VoiceLiveSessionAsyncClient` | Active WebSocket connection for streaming |\n| `VoiceLiveSessionOptions` | Configuration for session behavior |\n\n### Audio Requirements\n\n- **Sample Rate**: 24kHz (24000 Hz)\n- **Bit Depth**: 16-bit PCM\n- **Channels**: Mono (1 channel)\n- **Format**: Signed PCM, little-endian\n\n## Core Workflow\n\n### 1. Start Session\n\n```java\nimport reactor.core.publisher.Mono;\n\nclient.startSession(\"gpt-4o-realtime-preview\")\n    .flatMap(session -> {\n        System.out.println(\"Session started\");\n        \n        // Subscribe to events\n        session.receiveEvents()\n            .subscribe(\n                event -> System.out.println(\"Event: \" + event.getType()),\n                error -> System.err.println(\"Error: \" + error.getMessage())\n            );\n        \n        return Mono.just(session);\n    })\n    .block();\n```\n\n### 2. Configure Session Options\n\n```java\nimport com.azure.ai.voicelive.models.*;\nimport java.util.Arrays;\n\nServerVadTurnDetection turnDetection = new ServerVadTurnDetection()\n    .setThreshold(0.5)                    // Sensitivity (0.0-1.0)\n    .setPrefixPaddingMs(300)              // Audio before speech\n    .setSilenceDurationMs(500)            // Silence to end turn\n    .setInterruptResponse(true)           // Allow interruptions\n    .setAutoTruncate(true)\n    .setCreateResponse(true);\n\nAudioInputTranscriptionOptions transcription = new AudioInputTranscriptionOptions(\n    AudioInputTranscriptionOptionsModel.WHISPER_1);\n\nVoiceLiveSessionOptions options = new VoiceLiveSessionOptions()\n    .setInstructions(\"You are a helpful AI voice assistant.\")\n    .setVoice(BinaryData.fromObject(new OpenAIVoice(OpenAIVoiceName.ALLOY)))\n    .setModalities(Arrays.asList(InteractionModality.TEXT, InteractionModality.AUDIO))\n    .setInputAudioFormat(InputAudioFormat.PCM16)\n    .setOutputAudioFormat(OutputAudioFormat.PCM16)\n    .setInputAudioSamplingRate(24000)\n    .setInputAudioNoiseReduction(new AudioNoiseReduction(AudioNoiseReductionType.NEAR_FIELD))\n    .setInputAudioEchoCancellation(new AudioEchoCancellation())\n    .setInputAudioTranscription(transcription)\n    .setTurnDetection(turnDetection);\n\n// Send configuration\nClientEventSessionUpdate updateEvent = new ClientEventSessionUpdate(options);\nsession.sendEvent(updateEvent).subscribe();\n```\n\n### 3. Send Audio Input\n\n```java\nbyte[] audioData = readAudioChunk(); // Your PCM16 audio data\nsession.sendInputAudio(BinaryData.fromBytes(audioData)).subscribe();\n```\n\n### 4. Handle Events\n\n```java\nsession.receiveEvents().subscribe(event -> {\n    ServerEventType eventType = event.getType();\n    \n    if (ServerEventType.SESSION_CREATED.equals(eventType)) {\n        System.out.println(\"Session created\");\n    } else if (ServerEventType.INPUT_AUDIO_BUFFER_SPEECH_STARTED.equals(eventType)) {\n        System.out.println(\"User started speaking\");\n    } else if (ServerEventType.INPUT_AUDIO_BUFFER_SPEECH_STOPPED.equals(eventType)) {\n        System.out.println(\"User stopped speaking\");\n    } else if (ServerEventType.RESPONSE_AUDIO_DELTA.equals(eventType)) {\n        if (event instanceof SessionUpdateResponseAudioDelta) {\n            SessionUpdateResponseAudioDelta audioEvent = (SessionUpdateResponseAudioDelta) event;\n            playAudioChunk(audioEvent.getDelta());\n        }\n    } else if (ServerEventType.RESPONSE_DONE.equals(eventType)) {\n        System.out.println(\"Response complete\");\n    } else if (ServerEventType.ERROR.equals(eventType)) {\n        if (event instanceof SessionUpdateError) {\n            SessionUpdateError errorEvent = (SessionUpdateError) event;\n            System.err.println(\"Error: \" + errorEvent.getError().getMessage());\n        }\n    }\n});\n```\n\n## ",
  "tags": [
    "react",
    "api",
    "ai",
    "gpt",
    "workflow",
    "azure"
  ],
  "useCases": [],
  "scrapedAt": "2026-02-12T07:15:36.166Z"
}
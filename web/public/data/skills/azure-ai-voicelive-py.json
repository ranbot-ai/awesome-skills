{
  "id": "antigravity-azure-ai-voicelive-py",
  "name": "azure-ai-voicelive-py",
  "slug": "azure-ai-voicelive-py",
  "description": "Build real-time voice AI applications using Azure AI Voice Live SDK (azure-ai-voicelive). Use this skill when creating Python applications that need real-time bidirectional audio communication with Azure AI, including voice assistants, voice-enabled chatbots, real-time speech-to-speech translation, ",
  "category": "AI & Agents",
  "source": "antigravity",
  "repoUrl": "https://github.com/sickn33/antigravity-awesome-skills",
  "skillUrl": "https://github.com/sickn33/antigravity-awesome-skills/tree/main/skills/azure-ai-voicelive-py",
  "content": "\n# Azure AI Voice Live SDK\n\nBuild real-time voice AI applications with bidirectional WebSocket communication.\n\n## Installation\n\n```bash\npip install azure-ai-voicelive aiohttp azure-identity\n```\n\n## Environment Variables\n\n```bash\nAZURE_COGNITIVE_SERVICES_ENDPOINT=https://<region>.api.cognitive.microsoft.com\n# For API key auth (not recommended for production)\nAZURE_COGNITIVE_SERVICES_KEY=<api-key>\n```\n\n## Authentication\n\n**DefaultAzureCredential (preferred)**:\n```python\nfrom azure.ai.voicelive.aio import connect\nfrom azure.identity.aio import DefaultAzureCredential\n\nasync with connect(\n    endpoint=os.environ[\"AZURE_COGNITIVE_SERVICES_ENDPOINT\"],\n    credential=DefaultAzureCredential(),\n    model=\"gpt-4o-realtime-preview\",\n    credential_scopes=[\"https://cognitiveservices.azure.com/.default\"]\n) as conn:\n    ...\n```\n\n**API Key**:\n```python\nfrom azure.ai.voicelive.aio import connect\nfrom azure.core.credentials import AzureKeyCredential\n\nasync with connect(\n    endpoint=os.environ[\"AZURE_COGNITIVE_SERVICES_ENDPOINT\"],\n    credential=AzureKeyCredential(os.environ[\"AZURE_COGNITIVE_SERVICES_KEY\"]),\n    model=\"gpt-4o-realtime-preview\"\n) as conn:\n    ...\n```\n\n## Quick Start\n\n```python\nimport asyncio\nimport os\nfrom azure.ai.voicelive.aio import connect\nfrom azure.identity.aio import DefaultAzureCredential\n\nasync def main():\n    async with connect(\n        endpoint=os.environ[\"AZURE_COGNITIVE_SERVICES_ENDPOINT\"],\n        credential=DefaultAzureCredential(),\n        model=\"gpt-4o-realtime-preview\",\n        credential_scopes=[\"https://cognitiveservices.azure.com/.default\"]\n    ) as conn:\n        # Update session with instructions\n        await conn.session.update(session={\n            \"instructions\": \"You are a helpful assistant.\",\n            \"modalities\": [\"text\", \"audio\"],\n            \"voice\": \"alloy\"\n        })\n        \n        # Listen for events\n        async for event in conn:\n            print(f\"Event: {event.type}\")\n            if event.type == \"response.audio_transcript.done\":\n                print(f\"Transcript: {event.transcript}\")\n            elif event.type == \"response.done\":\n                break\n\nasyncio.run(main())\n```\n\n## Core Architecture\n\n### Connection Resources\n\nThe `VoiceLiveConnection` exposes these resources:\n\n| Resource | Purpose | Key Methods |\n|----------|---------|-------------|\n| `conn.session` | Session configuration | `update(session=...)` |\n| `conn.response` | Model responses | `create()`, `cancel()` |\n| `conn.input_audio_buffer` | Audio input | `append()`, `commit()`, `clear()` |\n| `conn.output_audio_buffer` | Audio output | `clear()` |\n| `conn.conversation` | Conversation state | `item.create()`, `item.delete()`, `item.truncate()` |\n| `conn.transcription_session` | Transcription config | `update(session=...)` |\n\n## Session Configuration\n\n```python\nfrom azure.ai.voicelive.models import RequestSession, FunctionTool\n\nawait conn.session.update(session=RequestSession(\n    instructions=\"You are a helpful voice assistant.\",\n    modalities=[\"text\", \"audio\"],\n    voice=\"alloy\",  # or \"echo\", \"shimmer\", \"sage\", etc.\n    input_audio_format=\"pcm16\",\n    output_audio_format=\"pcm16\",\n    turn_detection={\n        \"type\": \"server_vad\",\n        \"threshold\": 0.5,\n        \"prefix_padding_ms\": 300,\n        \"silence_duration_ms\": 500\n    },\n    tools=[\n        FunctionTool(\n            type=\"function\",\n            name=\"get_weather\",\n            description=\"Get current weather\",\n            parameters={\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\"type\": \"string\"}\n                },\n                \"required\": [\"location\"]\n            }\n        )\n    ]\n))\n```\n\n## Audio Streaming\n\n### Send Audio (Base64 PCM16)\n\n```python\nimport base64\n\n# Read audio chunk (16-bit PCM, 24kHz mono)\naudio_chunk = await read_audio_from_microphone()\nb64_audio = base64.b64encode(audio_chunk).decode()\n\nawait conn.input_audio_buffer.append(audio=b64_audio)\n```\n\n### Receive Audio\n\n```python\nasync for event in conn:\n    if event.type == \"response.audio.delta\":\n        audio_bytes = base64.b64decode(event.delta)\n        await play_audio(audio_bytes)\n    elif event.type == \"response.audio.done\":\n        print(\"Audio complete\")\n```\n\n## Event Handling\n\n```python\nasync for event in conn:\n    match event.type:\n        # Session events\n        case \"session.created\":\n            print(f\"Session: {event.session}\")\n        case \"session.updated\":\n            print(\"Session updated\")\n        \n        # Audio input events\n        case \"input_audio_buffer.speech_started\":\n            print(f\"Speech started at {event.audio_start_ms}ms\")\n        case \"input_audio_buffer.speech_stopped\":\n            print(f\"Speech stopped at {event.audio_end_ms}ms\")\n        \n        # Transcription events\n        case \"conversation.item.input_audio_transcription.completed\":\n            print(f\"User said: {event.transcript}\")\n        case \"conversation.item.input_audio_transcription.delta\":\n            print(f\"Pa",
  "tags": [
    "python",
    "api",
    "mcp",
    "ai",
    "gpt",
    "azure",
    "cro"
  ],
  "useCases": [],
  "scrapedAt": "2026-02-12T07:15:36.463Z"
}
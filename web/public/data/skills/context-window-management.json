{
  "id": "antigravity-context-window-management",
  "name": "context-window-management",
  "slug": "context-window-management",
  "description": "Strategies for managing LLM context windows including summarization, trimming, routing, and avoiding context rot Use when: context window, token limit, context management, context engineering, long context.",
  "category": "AI & Agents",
  "source": "antigravity",
  "repoUrl": "https://github.com/sickn33/antigravity-awesome-skills",
  "skillUrl": "https://github.com/sickn33/antigravity-awesome-skills/tree/main/skills/context-window-management",
  "content": "\n# Context Window Management\n\nYou're a context engineering specialist who has optimized LLM applications handling\nmillions of conversations. You've seen systems hit token limits, suffer context rot,\nand lose critical information mid-dialogue.\n\nYou understand that context is a finite resource with diminishing returns. More tokens\ndoesn't mean better results—the art is in curating the right information. You know\nthe serial position effect, the lost-in-the-middle problem, and when to summarize\nversus when to retrieve.\n\nYour cor\n\n## Capabilities\n\n- context-engineering\n- context-summarization\n- context-trimming\n- context-routing\n- token-counting\n- context-prioritization\n\n## Patterns\n\n### Tiered Context Strategy\n\nDifferent strategies based on context size\n\n### Serial Position Optimization\n\nPlace important content at start and end\n\n### Intelligent Summarization\n\nSummarize by importance, not just recency\n\n## Anti-Patterns\n\n### ❌ Naive Truncation\n\n### ❌ Ignoring Token Costs\n\n### ❌ One-Size-Fits-All\n\n## Related Skills\n\nWorks well with: `rag-implementation`, `conversation-memory`, `prompt-caching`, `llm-npc-dialogue`\n",
  "tags": [
    "ai",
    "llm",
    "rag"
  ],
  "useCases": [],
  "scrapedAt": "2026-01-26T13:17:49.263Z"
}
{
  "id": "composio-firecrawl-automation",
  "name": "Firecrawl Automation",
  "slug": "firecrawl-automation",
  "description": "Automate web crawling and data extraction with Firecrawl -- scrape pages, crawl sites, extract structured data, batch scrape URLs, and map website structures through the Composio Firecrawl integration.",
  "category": "Development & Code Tools",
  "source": "composio",
  "repoUrl": "https://github.com/ComposioHQ/awesome-claude-skills",
  "skillUrl": "https://github.com/ComposioHQ/awesome-claude-skills/tree/master/firecrawl-automation",
  "content": "\n# Firecrawl Automation\n\nRun **Firecrawl** web crawling and extraction directly from Claude Code. Scrape individual pages, crawl entire sites, extract structured data with AI, batch process URL lists, and map website structures without leaving your terminal.\n\n**Toolkit docs:** [composio.dev/toolkits/firecrawl](https://composio.dev/toolkits/firecrawl)\n\n---\n\n## Setup\n\n1. Add the Composio MCP server to your configuration:\n   ```\n   https://rube.app/mcp\n   ```\n2. Connect your Firecrawl account when prompted. The agent will provide an authentication link.\n3. Be mindful of credit consumption -- scope your crawls tightly and test on small URL sets before scaling.\n\n---\n\n## Core Workflows\n\n### 1. Scrape a Single Page\n\nFetch content from a URL in multiple formats with optional browser actions for dynamic pages.\n\n**Tool:** `FIRECRAWL_SCRAPE`\n\nKey parameters:\n- `url` (required) -- fully qualified URL to scrape\n- `formats` -- output formats: `markdown` (default), `html`, `rawHtml`, `links`, `screenshot`, `json`\n- `onlyMainContent` (default true) -- extract main content only, excluding nav/footer/ads\n- `waitFor` -- milliseconds to wait for JS rendering (default 0)\n- `timeout` -- max wait in ms (default 30000)\n- `actions` -- browser actions before scraping (click, write, wait, press, scroll)\n- `includeTags` / `excludeTags` -- filter by HTML tags\n- `jsonOptions` -- for structured extraction with `schema` and/or `prompt`\n\nExample prompt: *\"Scrape the main content from https://example.com/pricing as markdown\"*\n\n---\n\n### 2. Crawl an Entire Site\n\nDiscover and scrape multiple pages from a website with configurable depth, path filters, and concurrency.\n\n**Tool:** `FIRECRAWL_CRAWL_V2`\n\nKey parameters:\n- `url` (required) -- starting URL for the crawl\n- `limit` (default 10) -- max pages to crawl\n- `maxDiscoveryDepth` -- depth limit from the root page\n- `includePaths` / `excludePaths` -- regex patterns for URL paths\n- `allowSubdomains` -- include subdomains (default false)\n- `crawlEntireDomain` -- follow sibling/parent links, not just children (default false)\n- `sitemap` -- `include` (default), `skip`, or `only`\n- `prompt` -- natural language to auto-configure crawler settings\n- `scrapeOptions_formats` -- output format for each page\n- `scrapeOptions_onlyMainContent` -- main content extraction per page\n\nExample prompt: *\"Crawl the docs section of firecrawl.dev, max 50 pages, only paths matching docs\"*\n\n---\n\n### 3. Extract Structured Data\n\nExtract structured JSON data from web pages using AI with a natural language prompt or JSON schema.\n\n**Tool:** `FIRECRAWL_EXTRACT`\n\nKey parameters:\n- `urls` (required) -- array of URLs to extract from (max 10 in beta). Supports wildcards like `https://example.com/blog/*`\n- `prompt` -- natural language description of what to extract\n- `schema` -- JSON Schema defining the desired output structure\n- `enable_web_search` -- allow crawling links outside initial domains (default false)\n\nAt least one of `prompt` or `schema` must be provided.\n\nCheck extraction status with `FIRECRAWL_EXTRACT_GET` using the returned job `id`.\n\nExample prompt: *\"Extract company name, pricing tiers, and feature lists from https://example.com/pricing\"*\n\n---\n\n### 4. Batch Scrape Multiple URLs\n\nScrape many URLs concurrently with shared configuration for efficient bulk data collection.\n\n**Tool:** `FIRECRAWL_BATCH_SCRAPE`\n\nKey parameters:\n- `urls` (required) -- array of URLs to scrape\n- `formats` -- output format for all pages (default `markdown`)\n- `onlyMainContent` (default true) -- main content extraction\n- `maxConcurrency` -- parallel scrape limit\n- `ignoreInvalidURLs` (default true) -- skip bad URLs instead of failing the batch\n- `location` -- geolocation settings with `country` code\n- `actions` -- browser actions applied to each page\n- `blockAds` (default true) -- block advertisements\n\nExample prompt: *\"Batch scrape these 20 product page URLs as markdown with ad blocking\"*\n\n---\n\n### 5. Map Website Structure\n\nDiscover all URLs on a website from a starting URL, useful for planning crawls or auditing site structure.\n\n**Tool:** `FIRECRAWL_MAP_MULTIPLE_URLS_BASED_ON_OPTIONS`\n\nKey parameters:\n- `url` (required) -- starting URL (must be `https://` or `http://`)\n- `search` -- guide URL discovery toward specific page types\n- `limit` (default 5000, max 100000) -- max URLs to return\n- `includeSubdomains` (default true) -- include subdomains\n- `ignoreQueryParameters` (default true) -- dedupe URLs differing only by query params\n- `sitemap` -- `include`, `skip`, or `only`\n\nExample prompt: *\"Map all URLs on docs.example.com, focusing on API reference pages\"*\n\n---\n\n### 6. Monitor and Manage Crawl Jobs\n\nTrack crawl progress, retrieve results, and cancel runaway jobs.\n\n**Tools:** `FIRECRAWL_CRAWL_GET`, `FIRECRAWL_GET_THE_STATUS_OF_A_CRAWL_JOB`, `FIRECRAWL_CANCEL_A_CRAWL_JOB`\n\n- `FIRECRAWL_CRAWL_GET` -- get status, progress, credits used, and crawled page data\n- `FIRECRAWL_CANCEL_A_CRAWL_JOB` -- stop an active or queued crawl\n\nBoth require the cr",
  "tags": [
    "html",
    "api",
    "markdown",
    "json",
    "cli",
    "mcp",
    "automation",
    "ai",
    "claude"
  ],
  "useCases": [],
  "scrapedAt": "2026-02-12T07:11:57.035Z"
}
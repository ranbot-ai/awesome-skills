{
  "id": "antigravity-llm-app-patterns",
  "name": "llm-app-patterns",
  "slug": "llm-app-patterns",
  "description": "Production-ready patterns for building LLM applications. Covers RAG pipelines, agent architectures, prompt IDEs, and LLMOps monitoring. Use when designing AI applications, implementing RAG, building agents, or setting up LLM observability.",
  "category": "AI & Agents",
  "source": "antigravity",
  "repoUrl": "https://github.com/sickn33/antigravity-awesome-skills",
  "skillUrl": "https://github.com/sickn33/antigravity-awesome-skills/tree/main/skills/llm-app-patterns",
  "content": "\n# ğŸ¤– LLM Application Patterns\n\n> Production-ready patterns for building LLM applications, inspired by [Dify](https://github.com/langgenius/dify) and industry best practices.\n\n## When to Use This Skill\n\nUse this skill when:\n\n- Designing LLM-powered applications\n- Implementing RAG (Retrieval-Augmented Generation)\n- Building AI agents with tools\n- Setting up LLMOps monitoring\n- Choosing between agent architectures\n\n---\n\n## 1. RAG Pipeline Architecture\n\n### Overview\n\nRAG (Retrieval-Augmented Generation) grounds LLM responses in your data.\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Ingest    â”‚â”€â”€â”€â”€â–¶â”‚   Retrieve  â”‚â”€â”€â”€â”€â–¶â”‚   Generate  â”‚\nâ”‚  Documents  â”‚     â”‚   Context   â”‚     â”‚   Response  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n      â”‚                   â”‚                   â”‚\n      â–¼                   â–¼                   â–¼\n â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n â”‚ Chunkingâ”‚       â”‚  Vector   â”‚       â”‚    LLM    â”‚\n â”‚Embeddingâ”‚       â”‚  Search   â”‚       â”‚  + Contextâ”‚\n â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### 1.1 Document Ingestion\n\n```python\n# Chunking strategies\nclass ChunkingStrategy:\n    # Fixed-size chunks (simple but may break context)\n    FIXED_SIZE = \"fixed_size\"  # e.g., 512 tokens\n\n    # Semantic chunking (preserves meaning)\n    SEMANTIC = \"semantic\"      # Split on paragraphs/sections\n\n    # Recursive splitting (tries multiple separators)\n    RECURSIVE = \"recursive\"    # [\"\\n\\n\", \"\\n\", \" \", \"\"]\n\n    # Document-aware (respects structure)\n    DOCUMENT_AWARE = \"document_aware\"  # Headers, lists, etc.\n\n# Recommended settings\nCHUNK_CONFIG = {\n    \"chunk_size\": 512,       # tokens\n    \"chunk_overlap\": 50,     # token overlap between chunks\n    \"separators\": [\"\\n\\n\", \"\\n\", \". \", \" \"],\n}\n```\n\n### 1.2 Embedding & Storage\n\n```python\n# Vector database selection\nVECTOR_DB_OPTIONS = {\n    \"pinecone\": {\n        \"use_case\": \"Production, managed service\",\n        \"scale\": \"Billions of vectors\",\n        \"features\": [\"Hybrid search\", \"Metadata filtering\"]\n    },\n    \"weaviate\": {\n        \"use_case\": \"Self-hosted, multi-modal\",\n        \"scale\": \"Millions of vectors\",\n        \"features\": [\"GraphQL API\", \"Modules\"]\n    },\n    \"chromadb\": {\n        \"use_case\": \"Development, prototyping\",\n        \"scale\": \"Thousands of vectors\",\n        \"features\": [\"Simple API\", \"In-memory option\"]\n    },\n    \"pgvector\": {\n        \"use_case\": \"Existing Postgres infrastructure\",\n        \"scale\": \"Millions of vectors\",\n        \"features\": [\"SQL integration\", \"ACID compliance\"]\n    }\n}\n\n# Embedding model selection\nEMBEDDING_MODELS = {\n    \"openai/text-embedding-3-small\": {\n        \"dimensions\": 1536,\n        \"cost\": \"$0.02/1M tokens\",\n        \"quality\": \"Good for most use cases\"\n    },\n    \"openai/text-embedding-3-large\": {\n        \"dimensions\": 3072,\n        \"cost\": \"$0.13/1M tokens\",\n        \"quality\": \"Best for complex queries\"\n    },\n    \"local/bge-large\": {\n        \"dimensions\": 1024,\n        \"cost\": \"Free (compute only)\",\n        \"quality\": \"Comparable to OpenAI small\"\n    }\n}\n```\n\n### 1.3 Retrieval Strategies\n\n```python\n# Basic semantic search\ndef semantic_search(query: str, top_k: int = 5):\n    query_embedding = embed(query)\n    results = vector_db.similarity_search(\n        query_embedding,\n        top_k=top_k\n    )\n    return results\n\n# Hybrid search (semantic + keyword)\ndef hybrid_search(query: str, top_k: int = 5, alpha: float = 0.5):\n    \"\"\"\n    alpha=1.0: Pure semantic\n    alpha=0.0: Pure keyword (BM25)\n    alpha=0.5: Balanced\n    \"\"\"\n    semantic_results = vector_db.similarity_search(query)\n    keyword_results = bm25_search(query)\n\n    # Reciprocal Rank Fusion\n    return rrf_merge(semantic_results, keyword_results, alpha)\n\n# Multi-query retrieval\ndef multi_query_retrieval(query: str):\n    \"\"\"Generate multiple query variations for better recall\"\"\"\n    queries = llm.generate_query_variations(query, n=3)\n    all_results = []\n    for q in queries:\n        all_results.extend(semantic_search(q))\n    return deduplicate(all_results)\n\n# Contextual compression\ndef compressed_retrieval(query: str):\n    \"\"\"Retrieve then compress to relevant parts only\"\"\"\n    docs = semantic_search(query, top_k=10)\n    compressed = llm.extract_relevant_parts(docs, query)\n    return compressed\n```\n\n### 1.4 Generation with Context\n\n```python\nRAG_PROMPT_TEMPLATE = \"\"\"\nAnswer the user's question based ONLY on the following context.\nIf the context doesn't contain enough information, say \"I don't have enough information to answer that.\"\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:\"\"\"\n\ndef generate_with_rag(question: str):\n    # Retrieve\n    context_docs = hybrid_search(question, top_k=5)\n    context = \"\\n\\n\".join([doc.content for doc in context_docs])\n\n    # Generate\n    prompt = RAG_PROMPT_TEMPLATE.format(\n        context=context,\n        question=question\n    )\n\n    response = llm.generate(prompt)\n\n    # Return with citations\n    return {\n        \"answer\": response,\n        \"sources\": [do",
  "tags": [
    "python",
    "react",
    "api",
    "claude",
    "ai",
    "agent",
    "llm",
    "gpt",
    "template",
    "design"
  ],
  "useCases": [
    "Designing LLM-powered applications",
    "Implementing RAG (Retrieval-Augmented Generation)",
    "Building AI agents with tools",
    "Setting up LLMOps monitoring",
    "Choosing between agent architectures"
  ],
  "scrapedAt": "2026-01-26T13:19:20.611Z"
}
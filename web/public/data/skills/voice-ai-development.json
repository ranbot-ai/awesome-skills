{
  "id": "antigravity-voice-ai-development",
  "name": "voice-ai-development",
  "slug": "voice-ai-development",
  "description": "Expert in building voice AI applications - from real-time voice agents to voice-enabled apps. Covers OpenAI Realtime API, Vapi for voice agents, Deepgram for transcription, ElevenLabs for synthesis, LiveKit for real-time infrastructure, and WebRTC fundamentals. Knows how to build low-latency, produc",
  "category": "Development & Code Tools",
  "source": "antigravity",
  "repoUrl": "https://github.com/sickn33/antigravity-awesome-skills",
  "skillUrl": "https://github.com/sickn33/antigravity-awesome-skills/tree/main/skills/voice-ai-development",
  "content": "\n# Voice AI Development\n\n**Role**: Voice AI Architect\n\nYou are an expert in building real-time voice applications. You think in terms of\nlatency budgets, audio quality, and user experience. You know that voice apps feel\nmagical when fast and broken when slow. You choose the right combination of providers\nfor each use case and optimize relentlessly for perceived responsiveness.\n\n## Capabilities\n\n- OpenAI Realtime API\n- Vapi voice agents\n- Deepgram STT/TTS\n- ElevenLabs voice synthesis\n- LiveKit real-time infrastructure\n- WebRTC audio handling\n- Voice agent design\n- Latency optimization\n\n## Requirements\n\n- Python or Node.js\n- API keys for providers\n- Audio handling knowledge\n\n## Patterns\n\n### OpenAI Realtime API\n\nNative voice-to-voice with GPT-4o\n\n**When to use**: When you want integrated voice AI without separate STT/TTS\n\n```python\nimport asyncio\nimport websockets\nimport json\nimport base64\n\nOPENAI_API_KEY = \"sk-...\"\n\nasync def voice_session():\n    url = \"wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview\"\n    headers = {\n        \"Authorization\": f\"Bearer {OPENAI_API_KEY}\",\n        \"OpenAI-Beta\": \"realtime=v1\"\n    }\n\n    async with websockets.connect(url, extra_headers=headers) as ws:\n        # Configure session\n        await ws.send(json.dumps({\n            \"type\": \"session.update\",\n            \"session\": {\n                \"modalities\": [\"text\", \"audio\"],\n                \"voice\": \"alloy\",  # alloy, echo, fable, onyx, nova, shimmer\n                \"input_audio_format\": \"pcm16\",\n                \"output_audio_format\": \"pcm16\",\n                \"input_audio_transcription\": {\n                    \"model\": \"whisper-1\"\n                },\n                \"turn_detection\": {\n                    \"type\": \"server_vad\",  # Voice activity detection\n                    \"threshold\": 0.5,\n                    \"prefix_padding_ms\": 300,\n                    \"silence_duration_ms\": 500\n                },\n                \"tools\": [\n                    {\n                        \"type\": \"function\",\n                        \"name\": \"get_weather\",\n                        \"description\": \"Get weather for a location\",\n                        \"parameters\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"location\": {\"type\": \"string\"}\n                            }\n                        }\n                    }\n                ]\n            }\n        }))\n\n        # Send audio (PCM16, 24kHz, mono)\n        async def send_audio(audio_bytes):\n            await ws.send(json.dumps({\n                \"type\": \"input_audio_buffer.append\",\n                \"audio\": base64.b64encode(audio_bytes).decode()\n            }))\n\n        # Receive events\n        async for message in ws:\n            event = json.loads(message)\n\n            if event[\"type\"] == \"resp\n```\n\n### Vapi Voice Agent\n\nBuild voice agents with Vapi platform\n\n**When to use**: Phone-based agents, quick deployment\n\n```python\n# Vapi provides hosted voice agents with webhooks\n\nfrom flask import Flask, request, jsonify\nimport vapi\n\napp = Flask(__name__)\nclient = vapi.Vapi(api_key=\"...\")\n\n# Create an assistant\nassistant = client.assistants.create(\n    name=\"Support Agent\",\n    model={\n        \"provider\": \"openai\",\n        \"model\": \"gpt-4o\",\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful support agent...\"\n            }\n        ]\n    },\n    voice={\n        \"provider\": \"11labs\",\n        \"voiceId\": \"21m00Tcm4TlvDq8ikWAM\"  # Rachel\n    },\n    firstMessage=\"Hi! How can I help you today?\",\n    transcriber={\n        \"provider\": \"deepgram\",\n        \"model\": \"nova-2\"\n    }\n)\n\n# Webhook for conversation events\n@app.route(\"/vapi/webhook\", methods=[\"POST\"])\ndef vapi_webhook():\n    event = request.json\n\n    if event[\"type\"] == \"function-call\":\n        # Handle tool call\n        name = event[\"functionCall\"][\"name\"]\n        args = event[\"functionCall\"][\"parameters\"]\n\n        if name == \"check_order\":\n            result = check_order(args[\"order_id\"])\n            return jsonify({\"result\": result})\n\n    elif event[\"type\"] == \"end-of-call-report\":\n        # Call ended - save transcript\n        transcript = event[\"transcript\"]\n        save_transcript(event[\"call\"][\"id\"], transcript)\n\n    return jsonify({\"ok\": True})\n\n# Start outbound call\ncall = client.calls.create(\n    assistant_id=assistant.id,\n    customer={\n        \"number\": \"+1234567890\"\n    },\n    phoneNumber={\n        \"twilioPhoneNumber\": \"+0987654321\"\n    }\n)\n\n# Or create web call\nweb_call = client.calls.create(\n    assistant_id=assistant.id,\n    type=\"web\"\n)\n# Returns URL for WebRTC connection\n```\n\n### Deepgram STT + ElevenLabs TTS\n\nBest-in-class transcription and synthesis\n\n**When to use**: High quality voice, custom pipeline\n\n```python\nimport asyncio\nfrom deepgram import DeepgramClient, LiveTranscriptionEvents\nfrom elevenlabs import ElevenLabs\n\n# Deepgram real-time transcription\ndeepgram = DeepgramClient(ap",
  "tags": [
    "python",
    "node",
    "api",
    "ai",
    "agent",
    "llm",
    "gpt",
    "design",
    "langgraph"
  ],
  "useCases": [],
  "scrapedAt": "2026-01-26T13:22:35.883Z"
}
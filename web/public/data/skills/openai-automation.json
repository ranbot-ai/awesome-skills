{
  "id": "composio-openai-automation",
  "name": "OpenAI Automation",
  "slug": "openai-automation",
  "description": "Automate OpenAI API operations -- generate responses with multimodal and structured output support, create embeddings, generate images, and list models via the Composio MCP integration.",
  "category": "Development & Code Tools",
  "source": "composio",
  "repoUrl": "https://github.com/ComposioHQ/awesome-claude-skills",
  "skillUrl": "https://github.com/ComposioHQ/awesome-claude-skills/tree/master/openai-automation",
  "content": "\n# OpenAI Automation\n\nAutomate your OpenAI API workflows -- generate text with the Responses API (including multimodal image+text inputs and structured JSON outputs), create embeddings for search and clustering, generate images with DALL-E and GPT Image models, and list available models.\n\n**Toolkit docs:** [composio.dev/toolkits/openai](https://composio.dev/toolkits/openai)\n\n---\n\n## Setup\n\n1. Add the Composio MCP server to your client: `https://rube.app/mcp`\n2. Connect your OpenAI account when prompted (API key authentication)\n3. Start using the workflows below\n\n---\n\n## Core Workflows\n\n### 1. Generate a Response (Text, Multimodal, Structured)\n\nUse `OPENAI_CREATE_RESPONSE` for one-shot model responses including text, image analysis, OCR, and structured JSON outputs.\n\n```\nTool: OPENAI_CREATE_RESPONSE\nInputs:\n  - model: string (required) -- e.g., \"gpt-5\", \"gpt-4o\", \"o3-mini\"\n  - input: string | array (required)\n    Simple: \"Explain quantum computing\"\n    Multimodal: [\n      { role: \"user\", content: [\n        { type: \"input_text\", text: \"What is in this image?\" },\n        { type: \"input_image\", image_url: { url: \"https://...\" } }\n      ]}\n    ]\n  - temperature: number (0-2, optional -- not supported with reasoning models)\n  - max_output_tokens: integer (optional)\n  - reasoning: { effort: \"none\" | \"minimal\" | \"low\" | \"medium\" | \"high\" }\n  - text: object (structured output config)\n    - format: { type: \"json_schema\", name: \"...\", schema: {...}, strict: true }\n  - tools: array (function, code_interpreter, file_search, web_search)\n  - tool_choice: \"auto\" | \"none\" | \"required\" | { type: \"function\", function: { name: \"...\" } }\n  - store: boolean (false to opt out of model distillation)\n  - stream: boolean\n```\n\n**Structured output example:** Set `text.format` to `{ type: \"json_schema\", name: \"person\", schema: { type: \"object\", properties: { name: { type: \"string\" }, age: { type: \"integer\" } }, required: [\"name\", \"age\"], additionalProperties: false }, strict: true }`.\n\n### 2. Create Embeddings\n\nUse `OPENAI_CREATE_EMBEDDINGS` for vector search, clustering, recommendations, and RAG pipelines.\n\n```\nTool: OPENAI_CREATE_EMBEDDINGS\nInputs:\n  - input: string | string[] | int[] | int[][] (required) -- max 8192 tokens, max 2048 items\n  - model: string (required) -- \"text-embedding-3-small\", \"text-embedding-3-large\", \"text-embedding-ada-002\"\n  - dimensions: integer (optional, only for text-embedding-3 and later)\n  - encoding_format: \"float\" | \"base64\" (default \"float\")\n  - user: string (optional, end-user ID for abuse monitoring)\n```\n\n### 3. Generate Images\n\nUse `OPENAI_CREATE_IMAGE` to create images from text prompts using GPT Image or DALL-E models.\n\n```\nTool: OPENAI_CREATE_IMAGE\nInputs:\n  - model: string (required) -- \"gpt-image-1\", \"gpt-image-1.5\", \"dall-e-3\", \"dall-e-2\"\n  - prompt: string (required) -- max 32000 chars (GPT Image), 4000 (DALL-E 3), 1000 (DALL-E 2)\n  - size: \"1024x1024\" | \"1536x1024\" | \"1024x1536\" | \"auto\" | \"256x256\" | \"512x512\" | \"1792x1024\" | \"1024x1792\"\n  - quality: \"standard\" | \"hd\" | \"auto\" | \"high\" | \"medium\" | \"low\"\n  - n: integer (1-10; DALL-E 3 supports n=1 only)\n  - background: \"transparent\" | \"opaque\" | \"auto\" (GPT Image models only)\n  - style: \"vivid\" | \"natural\" (DALL-E 3 only)\n  - user: string (optional)\n```\n\n### 4. List Available Models\n\nUse `OPENAI_LIST_MODELS` to discover which models are accessible with your API key.\n\n```\nTool: OPENAI_LIST_MODELS\nInputs: (none)\n```\n\n---\n\n## Known Pitfalls\n\n| Pitfall | Detail |\n|---------|--------|\n| DALL-E deprecation | DALL-E 2 and DALL-E 3 are deprecated and will stop being supported on 05/12/2026. Prefer GPT Image models. |\n| DALL-E 3 single image only | `OPENAI_CREATE_IMAGE` with DALL-E 3 only supports `n=1`. Use GPT Image models or DALL-E 2 for multiple images. |\n| Token limits for embeddings | Input must not exceed 8192 tokens per item and 2048 items per batch for embedding models. |\n| Reasoning model restrictions | `temperature` and `top_p` are not supported with reasoning models (o3-mini, etc.). Use `reasoning.effort` instead. |\n| Structured output strict mode | When `strict: true` in json_schema format, ALL schema properties must be listed in the `required` array. |\n| Prompt length varies by model | Image prompt max lengths differ: 32000 (GPT Image), 4000 (DALL-E 3), 1000 (DALL-E 2). |\n\n---\n\n## Quick Reference\n\n| Tool Slug | Description |\n|-----------|-------------|\n| `OPENAI_CREATE_RESPONSE` | Generate text/multimodal responses with structured output support |\n| `OPENAI_CREATE_EMBEDDINGS` | Create text embeddings for search, clustering, and RAG |\n| `OPENAI_CREATE_IMAGE` | Generate images from text prompts |\n| `OPENAI_LIST_MODELS` | List all models available to your API key |\n\n---\n\n*Powered by [Composio](https://composio.dev)*\n",
  "tags": [
    "api",
    "json",
    "cli",
    "mcp",
    "automation",
    "ai"
  ],
  "useCases": [],
  "scrapedAt": "2026-02-12T07:13:19.639Z"
}
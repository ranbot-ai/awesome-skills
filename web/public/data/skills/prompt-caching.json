{
  "id": "antigravity-prompt-caching",
  "name": "prompt-caching",
  "slug": "prompt-caching",
  "description": "Caching strategies for LLM prompts including Anthropic prompt caching, response caching, and CAG (Cache Augmented Generation) Use when: prompt caching, cache prompt, response cache, cag, cache augmented.",
  "category": "Document Processing",
  "source": "antigravity",
  "repoUrl": "https://github.com/sickn33/antigravity-awesome-skills",
  "skillUrl": "https://github.com/sickn33/antigravity-awesome-skills/tree/main/skills/prompt-caching",
  "content": "\n# Prompt Caching\n\nYou're a caching specialist who has reduced LLM costs by 90% through strategic caching.\nYou've implemented systems that cache at multiple levels: prompt prefixes, full responses,\nand semantic similarity matches.\n\nYou understand that LLM caching is different from traditional caching—prompts have\nprefixes that can be cached, responses vary with temperature, and semantic similarity\noften matters more than exact match.\n\nYour core principles:\n1. Cache at the right level—prefix, response, or both\n2. K\n\n## Capabilities\n\n- prompt-cache\n- response-cache\n- kv-cache\n- cag-patterns\n- cache-invalidation\n\n## Patterns\n\n### Anthropic Prompt Caching\n\nUse Claude's native prompt caching for repeated prefixes\n\n### Response Caching\n\nCache full LLM responses for identical or similar queries\n\n### Cache Augmented Generation (CAG)\n\nPre-cache documents in prompt instead of RAG retrieval\n\n## Anti-Patterns\n\n### ❌ Caching with High Temperature\n\n### ❌ No Cache Invalidation\n\n### ❌ Caching Everything\n\n## ⚠️ Sharp Edges\n\n| Issue | Severity | Solution |\n|-------|----------|----------|\n| Cache miss causes latency spike with additional overhead | high | // Optimize for cache misses, not just hits |\n| Cached responses become incorrect over time | high | // Implement proper cache invalidation |\n| Prompt caching doesn't work due to prefix changes | medium | // Structure prompts for optimal caching |\n\n## Related Skills\n\nWorks well with: `context-window-management`, `rag-implementation`, `conversation-memory`\n",
  "tags": [
    "claude",
    "llm",
    "document",
    "rag"
  ],
  "useCases": [],
  "scrapedAt": "2026-01-26T13:20:57.969Z"
}
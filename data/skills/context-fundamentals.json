{
  "id": "antigravity-context-fundamentals",
  "name": "context-fundamentals",
  "slug": "context-fundamentals",
  "description": "Understand what context is, why it matters, and the anatomy of context in agent systems",
  "category": "AI & Agents",
  "source": "antigravity",
  "repoUrl": "https://github.com/sickn33/antigravity-awesome-skills",
  "skillUrl": "https://github.com/sickn33/antigravity-awesome-skills/tree/main/skills/context-fundamentals",
  "content": "\n## When to Use This Skill\n\nUnderstand what context is, why it matters, and the anatomy of context in agent systems\n\nUse this skill when working with understand what context is, why it matters, and the anatomy of context in agent systems.\n# Context Engineering Fundamentals\n\nContext is the complete state available to a language model at inference time. It includes everything the model can attend to when generating responses: system instructions, tool definitions, retrieved documents, message history, and tool outputs. Understanding context fundamentals is prerequisite to effective context engineering.\n\n## When to Activate\n\nActivate this skill when:\n- Designing new agent systems or modifying existing architectures\n- Debugging unexpected agent behavior that may relate to context\n- Optimizing context usage to reduce token costs or improve performance\n- Onboarding new team members to context engineering concepts\n- Reviewing context-related design decisions\n\n## Core Concepts\n\nContext comprises several distinct components, each with different characteristics and constraints. The attention mechanism creates a finite budget that constrains effective context usage. Progressive disclosure manages this constraint by loading information only as needed. The engineering discipline is curating the smallest high-signal token set that achieves desired outcomes.\n\n## Detailed Topics\n\n### The Anatomy of Context\n\n**System Prompts**\nSystem prompts establish the agent's core identity, constraints, and behavioral guidelines. They are loaded once at session start and typically persist throughout the conversation. System prompts should be extremely clear and use simple, direct language at the right altitude for the agent.\n\nThe right altitude balances two failure modes. At one extreme, engineers hardcode complex brittle logic that creates fragility and maintenance burden. At the other extreme, engineers provide vague high-level guidance that fails to give concrete signals for desired outputs or falsely assumes shared context. The optimal altitude strikes a balance: specific enough to guide behavior effectively, yet flexible enough to provide strong heuristics.\n\nOrganize prompts into distinct sections using XML tagging or Markdown headers to delineate background information, instructions, tool guidance, and output description. The exact formatting matters less as models become more capable, but structural clarity remains valuable.\n\n**Tool Definitions**\nTool definitions specify the actions an agent can take. Each tool includes a name, description, parameters, and return format. Tool definitions live near the front of context after serialization, typically before or after the system prompt.\n\nTool descriptions collectively steer agent behavior. Poor descriptions force agents to guess; optimized descriptions include usage context, examples, and defaults. The consolidation principle states that if a human engineer cannot definitively say which tool should be used in a given situation, an agent cannot be expected to do better.\n\n**Retrieved Documents**\nRetrieved documents provide domain-specific knowledge, reference materials, or task-relevant information. Agents use retrieval augmented generation to pull relevant documents into context at runtime rather than pre-loading all possible information.\n\nThe just-in-time approach maintains lightweight identifiers (file paths, stored queries, web links) and uses these references to load data into context dynamically. This mirrors human cognition: we generally do not memorize entire corpuses of information but rather use external organization and indexing systems to retrieve relevant information on demand.\n\n**Message History**\nMessage history contains the conversation between the user and agent, including previous queries, responses, and reasoning. For long-running tasks, message history can grow to dominate context usage.\n\nMessage history serves as scratchpad memory where agents track progress, maintain task state, and preserve reasoning across turns. Effective management of message history is critical for long-horizon task completion.\n\n**Tool Outputs**\nTool outputs are the results of agent actions: file contents, search results, command execution output, API responses, and similar data. Tool outputs comprise the majority of tokens in typical agent trajectories, with research showing observations (tool outputs) can reach 83.9% of total context usage.\n\nTool outputs consume context whether they are relevant to current decisions or not. This creates pressure for strategies like observation masking, compaction, and selective tool result retention.\n\n### Context Windows and Attention Mechanics\n\n**The Attention Budget Constraint**\nLanguage models process tokens through attention mechanisms that create pairwise relationships between all tokens in context. For n tokens, this creates nÂ² relationships that must be computed and stored. As context length increases, the model's ability to capture these relationshi",
  "tags": [
    "python",
    "markdown",
    "api",
    "claude",
    "ai",
    "agent",
    "design",
    "document",
    "rag",
    "cro"
  ],
  "useCases": [],
  "scrapedAt": "2026-01-31T06:51:26.274Z"
}
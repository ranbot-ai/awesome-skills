{
  "id": "antigravity-azure-monitor-ingestion-py",
  "name": "azure-monitor-ingestion-py",
  "slug": "azure-monitor-ingestion-py",
  "description": "Azure Monitor Ingestion SDK for Python. Use for sending custom logs to Log Analytics workspace via Logs Ingestion API.\nTriggers: \"azure-monitor-ingestion\", \"LogsIngestionClient\", \"custom logs\", \"DCR\", \"data collection rule\", \"Log Analytics\".\n",
  "category": "AI & Agents",
  "source": "antigravity",
  "repoUrl": "https://github.com/sickn33/antigravity-awesome-skills",
  "skillUrl": "https://github.com/sickn33/antigravity-awesome-skills/tree/main/skills/azure-monitor-ingestion-py",
  "content": "\n# Azure Monitor Ingestion SDK for Python\n\nSend custom logs to Azure Monitor Log Analytics workspace using the Logs Ingestion API.\n\n## Installation\n\n```bash\npip install azure-monitor-ingestion\npip install azure-identity\n```\n\n## Environment Variables\n\n```bash\n# Data Collection Endpoint (DCE)\nAZURE_DCE_ENDPOINT=https://<dce-name>.<region>.ingest.monitor.azure.com\n\n# Data Collection Rule (DCR) immutable ID\nAZURE_DCR_RULE_ID=dcr-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n\n# Stream name from DCR\nAZURE_DCR_STREAM_NAME=Custom-MyTable_CL\n```\n\n## Prerequisites\n\nBefore using this SDK, you need:\n\n1. **Log Analytics Workspace** — Target for your logs\n2. **Data Collection Endpoint (DCE)** — Ingestion endpoint\n3. **Data Collection Rule (DCR)** — Defines schema and destination\n4. **Custom Table** — In Log Analytics (created via DCR or manually)\n\n## Authentication\n\n```python\nfrom azure.monitor.ingestion import LogsIngestionClient\nfrom azure.identity import DefaultAzureCredential\nimport os\n\nclient = LogsIngestionClient(\n    endpoint=os.environ[\"AZURE_DCE_ENDPOINT\"],\n    credential=DefaultAzureCredential()\n)\n```\n\n## Upload Custom Logs\n\n```python\nfrom azure.monitor.ingestion import LogsIngestionClient\nfrom azure.identity import DefaultAzureCredential\nimport os\n\nclient = LogsIngestionClient(\n    endpoint=os.environ[\"AZURE_DCE_ENDPOINT\"],\n    credential=DefaultAzureCredential()\n)\n\nrule_id = os.environ[\"AZURE_DCR_RULE_ID\"]\nstream_name = os.environ[\"AZURE_DCR_STREAM_NAME\"]\n\nlogs = [\n    {\"TimeGenerated\": \"2024-01-15T10:00:00Z\", \"Computer\": \"server1\", \"Message\": \"Application started\"},\n    {\"TimeGenerated\": \"2024-01-15T10:01:00Z\", \"Computer\": \"server1\", \"Message\": \"Processing request\"},\n    {\"TimeGenerated\": \"2024-01-15T10:02:00Z\", \"Computer\": \"server2\", \"Message\": \"Connection established\"}\n]\n\nclient.upload(rule_id=rule_id, stream_name=stream_name, logs=logs)\n```\n\n## Upload from JSON File\n\n```python\nimport json\n\nwith open(\"logs.json\", \"r\") as f:\n    logs = json.load(f)\n\nclient.upload(rule_id=rule_id, stream_name=stream_name, logs=logs)\n```\n\n## Custom Error Handling\n\nHandle partial failures with a callback:\n\n```python\nfailed_logs = []\n\ndef on_error(error):\n    print(f\"Upload failed: {error.error}\")\n    failed_logs.extend(error.failed_logs)\n\nclient.upload(\n    rule_id=rule_id,\n    stream_name=stream_name,\n    logs=logs,\n    on_error=on_error\n)\n\n# Retry failed logs\nif failed_logs:\n    print(f\"Retrying {len(failed_logs)} failed logs...\")\n    client.upload(rule_id=rule_id, stream_name=stream_name, logs=failed_logs)\n```\n\n## Ignore Errors\n\n```python\ndef ignore_errors(error):\n    pass  # Silently ignore upload failures\n\nclient.upload(\n    rule_id=rule_id,\n    stream_name=stream_name,\n    logs=logs,\n    on_error=ignore_errors\n)\n```\n\n## Async Client\n\n```python\nimport asyncio\nfrom azure.monitor.ingestion.aio import LogsIngestionClient\nfrom azure.identity.aio import DefaultAzureCredential\n\nasync def upload_logs():\n    async with LogsIngestionClient(\n        endpoint=endpoint,\n        credential=DefaultAzureCredential()\n    ) as client:\n        await client.upload(\n            rule_id=rule_id,\n            stream_name=stream_name,\n            logs=logs\n        )\n\nasyncio.run(upload_logs())\n```\n\n## Sovereign Clouds\n\n```python\nfrom azure.identity import AzureAuthorityHosts, DefaultAzureCredential\nfrom azure.monitor.ingestion import LogsIngestionClient\n\n# Azure Government\ncredential = DefaultAzureCredential(authority=AzureAuthorityHosts.AZURE_GOVERNMENT)\nclient = LogsIngestionClient(\n    endpoint=\"https://example.ingest.monitor.azure.us\",\n    credential=credential,\n    credential_scopes=[\"https://monitor.azure.us/.default\"]\n)\n```\n\n## Batching Behavior\n\nThe SDK automatically:\n- Splits logs into chunks of 1MB or less\n- Compresses each chunk with gzip\n- Uploads chunks in parallel\n\nNo manual batching needed for large log sets.\n\n## Client Types\n\n| Client | Purpose |\n|--------|---------|\n| `LogsIngestionClient` | Sync client for uploading logs |\n| `LogsIngestionClient` (aio) | Async client for uploading logs |\n\n## Key Concepts\n\n| Concept | Description |\n|---------|-------------|\n| **DCE** | Data Collection Endpoint — ingestion URL |\n| **DCR** | Data Collection Rule — defines schema, transformations, destination |\n| **Stream** | Named data flow within a DCR |\n| **Custom Table** | Target table in Log Analytics (ends with `_CL`) |\n\n## DCR Stream Name Format\n\nStream names follow patterns:\n- `Custom-<TableName>_CL` — For custom tables\n- `Microsoft-<TableName>` — For built-in tables\n\n## Best Practices\n\n1. **Use DefaultAzureCredential** for authentication\n2. **Handle errors gracefully** — use `on_error` callback for partial failures\n3. **Include TimeGenerated** — Required field for all logs\n4. **Match DCR schema** — Log fields must match DCR column definitions\n5. **Use async client** for high-throughput scenarios\n6. **Batch uploads** — SDK handles batching, but send reasonable chunks\n7. **Monitor ingestion** — Check Log Analytics for ingestion status\n8. **Use context ma",
  "tags": [
    "python",
    "api",
    "ai",
    "azure",
    "cro"
  ],
  "useCases": [],
  "scrapedAt": "2026-02-12T07:15:53.075Z"
}
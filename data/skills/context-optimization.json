{
  "id": "antigravity-context-optimization",
  "name": "context-optimization",
  "slug": "context-optimization",
  "description": "Apply compaction, masking, and caching strategies",
  "category": "AI & Agents",
  "source": "antigravity",
  "repoUrl": "https://github.com/sickn33/antigravity-awesome-skills",
  "skillUrl": "https://github.com/sickn33/antigravity-awesome-skills/tree/main/skills/context-optimization",
  "content": "\n## When to Use This Skill\n\nApply compaction, masking, and caching strategies\n\nUse this skill when working with apply compaction, masking, and caching strategies.\n# Context Optimization Techniques\n\nContext optimization extends the effective capacity of limited context windows through strategic compression, masking, caching, and partitioning. The goal is not to magically increase context windows but to make better use of available capacity. Effective optimization can double or triple effective context capacity without requiring larger models or longer contexts.\n\n## When to Activate\n\nActivate this skill when:\n- Context limits constrain task complexity\n- Optimizing for cost reduction (fewer tokens = lower costs)\n- Reducing latency for long conversations\n- Implementing long-running agent systems\n- Needing to handle larger documents or conversations\n- Building production systems at scale\n\n## Core Concepts\n\nContext optimization extends effective capacity through four primary strategies: compaction (summarizing context near limits), observation masking (replacing verbose outputs with references), KV-cache optimization (reusing cached computations), and context partitioning (splitting work across isolated contexts).\n\nThe key insight is that context quality matters more than quantity. Optimization preserves signal while reducing noise. The art lies in selecting what to keep versus what to discard, and when to apply each technique.\n\n## Detailed Topics\n\n### Compaction Strategies\n\n**What is Compaction**\nCompaction is the practice of summarizing context contents when approaching limits, then reinitializing a new context window with the summary. This distills the contents of a context window in a high-fidelity manner, enabling the agent to continue with minimal performance degradation.\n\nCompaction typically serves as the first lever in context optimization. The art lies in selecting what to keep versus what to discard.\n\n**Compaction Implementation**\nCompaction works by identifying sections that can be compressed, generating summaries that capture essential points, and replacing full content with summaries. Priority for compression goes to tool outputs (replace with summaries), old turns (summarize early conversation), retrieved docs (summarize if recent versions exist), and never compress system prompt.\n\n**Summary Generation**\nEffective summaries preserve different elements depending on message type:\n\nTool outputs: Preserve key findings, metrics, and conclusions. Remove verbose raw output.\n\nConversational turns: Preserve key decisions, commitments, and context shifts. Remove filler and back-and-forth.\n\nRetrieved documents: Preserve key facts and claims. Remove supporting evidence and elaboration.\n\n### Observation Masking\n\n**The Observation Problem**\nTool outputs can comprise 80%+ of token usage in agent trajectories. Much of this is verbose output that has already served its purpose. Once an agent has used a tool output to make a decision, keeping the full output provides diminishing value while consuming significant context.\n\nObservation masking replaces verbose tool outputs with compact references. The information remains accessible if needed but does not consume context continuously.\n\n**Masking Strategy Selection**\nNot all observations should be masked equally:\n\nNever mask: Observations critical to current task, observations from the most recent turn, observations used in active reasoning.\n\nConsider masking: Observations from 3+ turns ago, verbose outputs with key points extractable, observations whose purpose has been served.\n\nAlways mask: Repeated outputs, boilerplate headers/footers, outputs already summarized in conversation.\n\n### KV-Cache Optimization\n\n**Understanding KV-Cache**\nThe KV-cache stores Key and Value tensors computed during inference, growing linearly with sequence length. Caching the KV-cache across requests sharing identical prefixes avoids recomputation.\n\nPrefix caching reuses KV blocks across requests with identical prefixes using hash-based block matching. This dramatically reduces cost and latency for requests with common prefixes like system prompts.\n\n**Cache Optimization Patterns**\nOptimize for caching by reordering context elements to maximize cache hits. Place stable elements first (system prompt, tool definitions), then frequently reused elements, then unique elements last.\n\nDesign prompts to maximize cache stability: avoid dynamic content like timestamps, use consistent formatting, keep structure stable across sessions.\n\n### Context Partitioning\n\n**Sub-Agent Partitioning**\nThe most aggressive form of context optimization is partitioning work across sub-agents with isolated contexts. Each sub-agent operates in a clean context focused on its subtask without carrying accumulated context from other subtasks.\n\nThis approach achieves separation of concernsâ€”the detailed search context remains isolated within sub-agents while the coordinator focuses on synthesis and analysis.\n\n**Result Aggregation",
  "tags": [
    "python",
    "ai",
    "agent",
    "template",
    "design",
    "document",
    "cro"
  ],
  "useCases": [],
  "scrapedAt": "2026-01-31T06:51:27.783Z"
}
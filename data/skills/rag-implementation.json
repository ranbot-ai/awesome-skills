{
  "id": "antigravity-rag-implementation",
  "name": "rag-implementation",
  "slug": "rag-implementation",
  "description": "Build Retrieval-Augmented Generation (RAG) systems for LLM applications with vector databases and semantic search. Use when implementing knowledge-grounded AI, building document Q&A systems, or integrating LLMs with external knowledge bases.",
  "category": "Document Processing",
  "source": "antigravity",
  "repoUrl": "https://github.com/sickn33/antigravity-awesome-skills",
  "skillUrl": "https://github.com/sickn33/antigravity-awesome-skills/tree/main/skills/rag-implementation",
  "content": "\n# RAG Implementation\n\nMaster Retrieval-Augmented Generation (RAG) to build LLM applications that provide accurate, grounded responses using external knowledge sources.\n\n## Use this skill when\n\n- Building Q&A systems over proprietary documents\n- Creating chatbots with current, factual information\n- Implementing semantic search with natural language queries\n- Reducing hallucinations with grounded responses\n- Enabling LLMs to access domain-specific knowledge\n- Building documentation assistants\n- Creating research tools with source citation\n\n## Do not use this skill when\n\n- You only need purely generative writing without retrieval\n- The dataset is too small to justify embeddings\n- You cannot store or process the source data safely\n\n## Instructions\n\n1. Define the corpus, update cadence, and evaluation targets.\n2. Choose embedding models and vector store based on scale.\n3. Build ingestion, chunking, and retrieval with reranking.\n4. Evaluate with grounded QA metrics and monitor drift.\n\n## Safety\n\n- Redact sensitive data and enforce access controls.\n- Avoid exposing source documents in responses when restricted.\n\n## Core Components\n\n### 1. Vector Databases\n**Purpose**: Store and retrieve document embeddings efficiently\n\n**Options:**\n- **Pinecone**: Managed, scalable, fast queries\n- **Weaviate**: Open-source, hybrid search\n- **Milvus**: High performance, on-premise\n- **Chroma**: Lightweight, easy to use\n- **Qdrant**: Fast, filtered search\n- **FAISS**: Meta's library, local deployment\n\n### 2. Embeddings\n**Purpose**: Convert text to numerical vectors for similarity search\n\n**Models:**\n- **text-embedding-ada-002** (OpenAI): General purpose, 1536 dims\n- **all-MiniLM-L6-v2** (Sentence Transformers): Fast, lightweight\n- **e5-large-v2**: High quality, multilingual\n- **Instructor**: Task-specific instructions\n- **bge-large-en-v1.5**: SOTA performance\n\n### 3. Retrieval Strategies\n**Approaches:**\n- **Dense Retrieval**: Semantic similarity via embeddings\n- **Sparse Retrieval**: Keyword matching (BM25, TF-IDF)\n- **Hybrid Search**: Combine dense + sparse\n- **Multi-Query**: Generate multiple query variations\n- **HyDE**: Generate hypothetical documents\n\n### 4. Reranking\n**Purpose**: Improve retrieval quality by reordering results\n\n**Methods:**\n- **Cross-Encoders**: BERT-based reranking\n- **Cohere Rerank**: API-based reranking\n- **Maximal Marginal Relevance (MMR)**: Diversity + relevance\n- **LLM-based**: Use LLM to score relevance\n\n## Quick Start\n\n```python\nfrom langchain.document_loaders import DirectoryLoader\nfrom langchain.text_splitters import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import OpenAI\n\n# 1. Load documents\nloader = DirectoryLoader('./docs', glob=\"**/*.txt\")\ndocuments = loader.load()\n\n# 2. Split into chunks\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200,\n    length_function=len\n)\nchunks = text_splitter.split_documents(documents)\n\n# 3. Create embeddings and vector store\nembeddings = OpenAIEmbeddings()\nvectorstore = Chroma.from_documents(chunks, embeddings)\n\n# 4. Create retrieval chain\nqa_chain = RetrievalQA.from_chain_type(\n    llm=OpenAI(),\n    chain_type=\"stuff\",\n    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 4}),\n    return_source_documents=True\n)\n\n# 5. Query\nresult = qa_chain({\"query\": \"What are the main features?\"})\nprint(result['result'])\nprint(result['source_documents'])\n```\n\n## Advanced RAG Patterns\n\n### Pattern 1: Hybrid Search\n```python\nfrom langchain.retrievers import BM25Retriever, EnsembleRetriever\n\n# Sparse retriever (BM25)\nbm25_retriever = BM25Retriever.from_documents(chunks)\nbm25_retriever.k = 5\n\n# Dense retriever (embeddings)\nembedding_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n\n# Combine with weights\nensemble_retriever = EnsembleRetriever(\n    retrievers=[bm25_retriever, embedding_retriever],\n    weights=[0.3, 0.7]\n)\n```\n\n### Pattern 2: Multi-Query Retrieval\n```python\nfrom langchain.retrievers.multi_query import MultiQueryRetriever\n\n# Generate multiple query perspectives\nretriever = MultiQueryRetriever.from_llm(\n    retriever=vectorstore.as_retriever(),\n    llm=OpenAI()\n)\n\n# Single query → multiple variations → combined results\nresults = retriever.get_relevant_documents(\"What is the main topic?\")\n```\n\n### Pattern 3: Contextual Compression\n```python\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import LLMChainExtractor\n\ncompressor = LLMChainExtractor.from_llm(llm)\n\ncompression_retriever = ContextualCompressionRetriever(\n    base_compressor=compressor,\n    base_retriever=vectorstore.as_retriever()\n)\n\n# Returns only relevant parts of documents\ncompressed_docs = compression_retriever.get_relevant_documents(\"query\")\n```\n\n### Pattern 4: Parent Document Retriever\n```python\nfrom langchain.retrievers import ParentDocumentRetriever\n",
  "tags": [
    "python",
    "markdown",
    "api",
    "ai",
    "llm",
    "template",
    "document",
    "gcp",
    "langchain",
    "rag"
  ],
  "useCases": [],
  "scrapedAt": "2026-01-26T13:21:05.266Z"
}
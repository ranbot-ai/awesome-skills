{
  "id": "antigravity-langfuse",
  "name": "langfuse",
  "slug": "langfuse",
  "description": "Expert in Langfuse - the open-source LLM observability platform. Covers tracing, prompt management, evaluation, datasets, and integration with LangChain, LlamaIndex, and OpenAI. Essential for debugging, monitoring, and improving LLM applications in production. Use when: langfuse, llm observability, ",
  "category": "AI & Agents",
  "source": "antigravity",
  "repoUrl": "https://github.com/sickn33/antigravity-awesome-skills",
  "skillUrl": "https://github.com/sickn33/antigravity-awesome-skills/tree/main/skills/langfuse",
  "content": "\n# Langfuse\n\n**Role**: LLM Observability Architect\n\nYou are an expert in LLM observability and evaluation. You think in terms of\ntraces, spans, and metrics. You know that LLM applications need monitoring\njust like traditional software - but with different dimensions (cost, quality,\nlatency). You use data to drive prompt improvements and catch regressions.\n\n## Capabilities\n\n- LLM tracing and observability\n- Prompt management and versioning\n- Evaluation and scoring\n- Dataset management\n- Cost tracking\n- Performance monitoring\n- A/B testing prompts\n\n## Requirements\n\n- Python or TypeScript/JavaScript\n- Langfuse account (cloud or self-hosted)\n- LLM API keys\n\n## Patterns\n\n### Basic Tracing Setup\n\nInstrument LLM calls with Langfuse\n\n**When to use**: Any LLM application\n\n```python\nfrom langfuse import Langfuse\n\n# Initialize client\nlangfuse = Langfuse(\n    public_key=\"pk-...\",\n    secret_key=\"sk-...\",\n    host=\"https://cloud.langfuse.com\"  # or self-hosted URL\n)\n\n# Create a trace for a user request\ntrace = langfuse.trace(\n    name=\"chat-completion\",\n    user_id=\"user-123\",\n    session_id=\"session-456\",  # Groups related traces\n    metadata={\"feature\": \"customer-support\"},\n    tags=[\"production\", \"v2\"]\n)\n\n# Log a generation (LLM call)\ngeneration = trace.generation(\n    name=\"gpt-4o-response\",\n    model=\"gpt-4o\",\n    model_parameters={\"temperature\": 0.7},\n    input={\"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]},\n    metadata={\"attempt\": 1}\n)\n\n# Make actual LLM call\nresponse = openai.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n)\n\n# Complete the generation with output\ngeneration.end(\n    output=response.choices[0].message.content,\n    usage={\n        \"input\": response.usage.prompt_tokens,\n        \"output\": response.usage.completion_tokens\n    }\n)\n\n# Score the trace\ntrace.score(\n    name=\"user-feedback\",\n    value=1,  # 1 = positive, 0 = negative\n    comment=\"User clicked helpful\"\n)\n\n# Flush before exit (important in serverless)\nlangfuse.flush()\n```\n\n### OpenAI Integration\n\nAutomatic tracing with OpenAI SDK\n\n**When to use**: OpenAI-based applications\n\n```python\nfrom langfuse.openai import openai\n\n# Drop-in replacement for OpenAI client\n# All calls automatically traced\n\nresponse = openai.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    # Langfuse-specific parameters\n    name=\"greeting\",  # Trace name\n    session_id=\"session-123\",\n    user_id=\"user-456\",\n    tags=[\"test\"],\n    metadata={\"feature\": \"chat\"}\n)\n\n# Works with streaming\nstream = openai.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n    stream=True,\n    name=\"story-generation\"\n)\n\nfor chunk in stream:\n    print(chunk.choices[0].delta.content, end=\"\")\n\n# Works with async\nimport asyncio\nfrom langfuse.openai import AsyncOpenAI\n\nasync_client = AsyncOpenAI()\n\nasync def main():\n    response = await async_client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n        name=\"async-greeting\"\n    )\n```\n\n### LangChain Integration\n\nTrace LangChain applications\n\n**When to use**: LangChain-based applications\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langfuse.callback import CallbackHandler\n\n# Create Langfuse callback handler\nlangfuse_handler = CallbackHandler(\n    public_key=\"pk-...\",\n    secret_key=\"sk-...\",\n    host=\"https://cloud.langfuse.com\",\n    session_id=\"session-123\",\n    user_id=\"user-456\"\n)\n\n# Use with any LangChain component\nllm = ChatOpenAI(model=\"gpt-4o\")\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.\"),\n    (\"user\", \"{input}\")\n])\n\nchain = prompt | llm\n\n# Pass handler to invoke\nresponse = chain.invoke(\n    {\"input\": \"Hello\"},\n    config={\"callbacks\": [langfuse_handler]}\n)\n\n# Or set as default\nimport langchain\nlangchain.callbacks.manager.set_handler(langfuse_handler)\n\n# Then all calls are traced\nresponse = chain.invoke({\"input\": \"Hello\"})\n\n# Works with agents, retrievers, etc.\nfrom langchain.agents import create_openai_tools_agent\n\nagent = create_openai_tools_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools)\n\nresult = agent_executor.invoke(\n    {\"input\": \"What's the weather?\"},\n    config={\"callbacks\": [langfuse_handler]}\n)\n```\n\n## Anti-Patterns\n\n### ❌ Not Flushing in Serverless\n\n**Why bad**: Traces are batched.\nServerless may exit before flush.\nData is lost.\n\n**Instead**: Always call langfuse.flush() at end.\nUse context managers where available.\nConsider sync mode for critical traces.\n\n### ❌ Tracing Everything\n\n**Why bad**: Noisy traces.\nPerformance overhead.\nHard to find important info.\n\n**Instead**: Focus on: LLM calls, key logic, user actions.\nGroup related operations.\nUse meaningful span names.\n\n### ❌ No User/Session IDs\n\n**Why bad**: Can't debug specific users.\nCan't track sessions.\nAnalytics",
  "tags": [
    "python",
    "javascript",
    "typescript",
    "api",
    "ai",
    "agent",
    "llm",
    "gpt",
    "template",
    "langgraph"
  ],
  "useCases": [],
  "scrapedAt": "2026-01-26T13:19:11.428Z"
}
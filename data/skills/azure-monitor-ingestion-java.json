{
  "id": "antigravity-azure-monitor-ingestion-java",
  "name": "azure-monitor-ingestion-java",
  "slug": "azure-monitor-ingestion-java",
  "description": "Azure Monitor Ingestion SDK for Java. Send custom logs to Azure Monitor via Data Collection Rules (DCR) and Data Collection Endpoints (DCE).\nTriggers: \"LogsIngestionClient java\", \"azure monitor ingestion java\", \"custom logs java\", \"DCR java\", \"data collection rule java\".\n",
  "category": "AI & Agents",
  "source": "antigravity",
  "repoUrl": "https://github.com/sickn33/antigravity-awesome-skills",
  "skillUrl": "https://github.com/sickn33/antigravity-awesome-skills/tree/main/skills/azure-monitor-ingestion-java",
  "content": "\n# Azure Monitor Ingestion SDK for Java\n\nClient library for sending custom logs to Azure Monitor using the Logs Ingestion API via Data Collection Rules.\n\n## Installation\n\n```xml\n<dependency>\n    <groupId>com.azure</groupId>\n    <artifactId>azure-monitor-ingestion</artifactId>\n    <version>1.2.11</version>\n</dependency>\n```\n\nOr use Azure SDK BOM:\n\n```xml\n<dependencyManagement>\n    <dependencies>\n        <dependency>\n            <groupId>com.azure</groupId>\n            <artifactId>azure-sdk-bom</artifactId>\n            <version>{bom_version}</version>\n            <type>pom</type>\n            <scope>import</scope>\n        </dependency>\n    </dependencies>\n</dependencyManagement>\n\n<dependencies>\n    <dependency>\n        <groupId>com.azure</groupId>\n        <artifactId>azure-monitor-ingestion</artifactId>\n    </dependency>\n</dependencies>\n```\n\n## Prerequisites\n\n- Data Collection Endpoint (DCE)\n- Data Collection Rule (DCR)\n- Log Analytics workspace\n- Target table (custom or built-in: CommonSecurityLog, SecurityEvents, Syslog, WindowsEvents)\n\n## Environment Variables\n\n```bash\nDATA_COLLECTION_ENDPOINT=https://<dce-name>.<region>.ingest.monitor.azure.com\nDATA_COLLECTION_RULE_ID=dcr-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nSTREAM_NAME=Custom-MyTable_CL\n```\n\n## Client Creation\n\n### Synchronous Client\n\n```java\nimport com.azure.identity.DefaultAzureCredential;\nimport com.azure.identity.DefaultAzureCredentialBuilder;\nimport com.azure.monitor.ingestion.LogsIngestionClient;\nimport com.azure.monitor.ingestion.LogsIngestionClientBuilder;\n\nDefaultAzureCredential credential = new DefaultAzureCredentialBuilder().build();\n\nLogsIngestionClient client = new LogsIngestionClientBuilder()\n    .endpoint(\"<data-collection-endpoint>\")\n    .credential(credential)\n    .buildClient();\n```\n\n### Asynchronous Client\n\n```java\nimport com.azure.monitor.ingestion.LogsIngestionAsyncClient;\n\nLogsIngestionAsyncClient asyncClient = new LogsIngestionClientBuilder()\n    .endpoint(\"<data-collection-endpoint>\")\n    .credential(new DefaultAzureCredentialBuilder().build())\n    .buildAsyncClient();\n```\n\n## Key Concepts\n\n| Concept | Description |\n|---------|-------------|\n| Data Collection Endpoint (DCE) | Ingestion endpoint URL for your region |\n| Data Collection Rule (DCR) | Defines data transformation and routing to tables |\n| Stream Name | Target stream in the DCR (e.g., `Custom-MyTable_CL`) |\n| Log Analytics Workspace | Destination for ingested logs |\n\n## Core Operations\n\n### Upload Custom Logs\n\n```java\nimport java.util.List;\nimport java.util.ArrayList;\n\nList<Object> logs = new ArrayList<>();\nlogs.add(new MyLogEntry(\"2024-01-15T10:30:00Z\", \"INFO\", \"Application started\"));\nlogs.add(new MyLogEntry(\"2024-01-15T10:30:05Z\", \"DEBUG\", \"Processing request\"));\n\nclient.upload(\"<data-collection-rule-id>\", \"<stream-name>\", logs);\nSystem.out.println(\"Logs uploaded successfully\");\n```\n\n### Upload with Concurrency\n\nFor large log collections, enable concurrent uploads:\n\n```java\nimport com.azure.monitor.ingestion.models.LogsUploadOptions;\nimport com.azure.core.util.Context;\n\nList<Object> logs = getLargeLogs(); // Large collection\n\nLogsUploadOptions options = new LogsUploadOptions()\n    .setMaxConcurrency(3);\n\nclient.upload(\"<data-collection-rule-id>\", \"<stream-name>\", logs, options, Context.NONE);\n```\n\n### Upload with Error Handling\n\nHandle partial upload failures gracefully:\n\n```java\nLogsUploadOptions options = new LogsUploadOptions()\n    .setLogsUploadErrorConsumer(uploadError -> {\n        System.err.println(\"Upload error: \" + uploadError.getResponseException().getMessage());\n        System.err.println(\"Failed logs count: \" + uploadError.getFailedLogs().size());\n        \n        // Option 1: Log and continue\n        // Option 2: Throw to abort remaining uploads\n        // throw uploadError.getResponseException();\n    });\n\nclient.upload(\"<data-collection-rule-id>\", \"<stream-name>\", logs, options, Context.NONE);\n```\n\n### Async Upload with Reactor\n\n```java\nimport reactor.core.publisher.Mono;\n\nList<Object> logs = getLogs();\n\nasyncClient.upload(\"<data-collection-rule-id>\", \"<stream-name>\", logs)\n    .doOnSuccess(v -> System.out.println(\"Upload completed\"))\n    .doOnError(e -> System.err.println(\"Upload failed: \" + e.getMessage()))\n    .subscribe();\n```\n\n## Log Entry Model Example\n\n```java\npublic class MyLogEntry {\n    private String timeGenerated;\n    private String level;\n    private String message;\n    \n    public MyLogEntry(String timeGenerated, String level, String message) {\n        this.timeGenerated = timeGenerated;\n        this.level = level;\n        this.message = message;\n    }\n    \n    // Getters required for JSON serialization\n    public String getTimeGenerated() { return timeGenerated; }\n    public String getLevel() { return level; }\n    public String getMessage() { return message; }\n}\n```\n\n## Error Handling\n\n```java\nimport com.azure.core.exception.HttpResponseException;\n\ntry {\n    client.upload(ruleId, streamName, logs);\n} catch (HttpResponseException e) {\n    System",
  "tags": [
    "react",
    "api",
    "ai",
    "security",
    "azure",
    "cro"
  ],
  "useCases": [],
  "scrapedAt": "2026-02-12T07:15:52.791Z"
}
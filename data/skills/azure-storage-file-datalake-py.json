{
  "id": "antigravity-azure-storage-file-datalake-py",
  "name": "azure-storage-file-datalake-py",
  "slug": "azure-storage-file-datalake-py",
  "description": "Azure Data Lake Storage Gen2 SDK for Python. Use for hierarchical file systems, big data analytics, and file/directory operations.\nTriggers: \"data lake\", \"DataLakeServiceClient\", \"FileSystemClient\", \"ADLS Gen2\", \"hierarchical namespace\".\n",
  "category": "AI & Agents",
  "source": "antigravity",
  "repoUrl": "https://github.com/sickn33/antigravity-awesome-skills",
  "skillUrl": "https://github.com/sickn33/antigravity-awesome-skills/tree/main/skills/azure-storage-file-datalake-py",
  "content": "\n# Azure Data Lake Storage Gen2 SDK for Python\n\nHierarchical file system for big data analytics workloads.\n\n## Installation\n\n```bash\npip install azure-storage-file-datalake azure-identity\n```\n\n## Environment Variables\n\n```bash\nAZURE_STORAGE_ACCOUNT_URL=https://<account>.dfs.core.windows.net\n```\n\n## Authentication\n\n```python\nfrom azure.identity import DefaultAzureCredential\nfrom azure.storage.filedatalake import DataLakeServiceClient\n\ncredential = DefaultAzureCredential()\naccount_url = \"https://<account>.dfs.core.windows.net\"\n\nservice_client = DataLakeServiceClient(account_url=account_url, credential=credential)\n```\n\n## Client Hierarchy\n\n| Client | Purpose |\n|--------|---------|\n| `DataLakeServiceClient` | Account-level operations |\n| `FileSystemClient` | Container (file system) operations |\n| `DataLakeDirectoryClient` | Directory operations |\n| `DataLakeFileClient` | File operations |\n\n## File System Operations\n\n```python\n# Create file system (container)\nfile_system_client = service_client.create_file_system(\"myfilesystem\")\n\n# Get existing\nfile_system_client = service_client.get_file_system_client(\"myfilesystem\")\n\n# Delete\nservice_client.delete_file_system(\"myfilesystem\")\n\n# List file systems\nfor fs in service_client.list_file_systems():\n    print(fs.name)\n```\n\n## Directory Operations\n\n```python\nfile_system_client = service_client.get_file_system_client(\"myfilesystem\")\n\n# Create directory\ndirectory_client = file_system_client.create_directory(\"mydir\")\n\n# Create nested directories\ndirectory_client = file_system_client.create_directory(\"path/to/nested/dir\")\n\n# Get directory client\ndirectory_client = file_system_client.get_directory_client(\"mydir\")\n\n# Delete directory\ndirectory_client.delete_directory()\n\n# Rename/move directory\ndirectory_client.rename_directory(new_name=\"myfilesystem/newname\")\n```\n\n## File Operations\n\n### Upload File\n\n```python\n# Get file client\nfile_client = file_system_client.get_file_client(\"path/to/file.txt\")\n\n# Upload from local file\nwith open(\"local-file.txt\", \"rb\") as data:\n    file_client.upload_data(data, overwrite=True)\n\n# Upload bytes\nfile_client.upload_data(b\"Hello, Data Lake!\", overwrite=True)\n\n# Append data (for large files)\nfile_client.append_data(data=b\"chunk1\", offset=0, length=6)\nfile_client.append_data(data=b\"chunk2\", offset=6, length=6)\nfile_client.flush_data(12)  # Commit the data\n```\n\n### Download File\n\n```python\nfile_client = file_system_client.get_file_client(\"path/to/file.txt\")\n\n# Download all content\ndownload = file_client.download_file()\ncontent = download.readall()\n\n# Download to file\nwith open(\"downloaded.txt\", \"wb\") as f:\n    download = file_client.download_file()\n    download.readinto(f)\n\n# Download range\ndownload = file_client.download_file(offset=0, length=100)\n```\n\n### Delete File\n\n```python\nfile_client.delete_file()\n```\n\n## List Contents\n\n```python\n# List paths (files and directories)\nfor path in file_system_client.get_paths():\n    print(f\"{'DIR' if path.is_directory else 'FILE'}: {path.name}\")\n\n# List paths in directory\nfor path in file_system_client.get_paths(path=\"mydir\"):\n    print(path.name)\n\n# Recursive listing\nfor path in file_system_client.get_paths(path=\"mydir\", recursive=True):\n    print(path.name)\n```\n\n## File/Directory Properties\n\n```python\n# Get properties\nproperties = file_client.get_file_properties()\nprint(f\"Size: {properties.size}\")\nprint(f\"Last modified: {properties.last_modified}\")\n\n# Set metadata\nfile_client.set_metadata(metadata={\"processed\": \"true\"})\n```\n\n## Access Control (ACL)\n\n```python\n# Get ACL\nacl = directory_client.get_access_control()\nprint(f\"Owner: {acl['owner']}\")\nprint(f\"Permissions: {acl['permissions']}\")\n\n# Set ACL\ndirectory_client.set_access_control(\n    owner=\"user-id\",\n    permissions=\"rwxr-x---\"\n)\n\n# Update ACL entries\nfrom azure.storage.filedatalake import AccessControlChangeResult\ndirectory_client.update_access_control_recursive(\n    acl=\"user:user-id:rwx\"\n)\n```\n\n## Async Client\n\n```python\nfrom azure.storage.filedatalake.aio import DataLakeServiceClient\nfrom azure.identity.aio import DefaultAzureCredential\n\nasync def datalake_operations():\n    credential = DefaultAzureCredential()\n    \n    async with DataLakeServiceClient(\n        account_url=\"https://<account>.dfs.core.windows.net\",\n        credential=credential\n    ) as service_client:\n        file_system_client = service_client.get_file_system_client(\"myfilesystem\")\n        file_client = file_system_client.get_file_client(\"test.txt\")\n        \n        await file_client.upload_data(b\"async content\", overwrite=True)\n        \n        download = await file_client.download_file()\n        content = await download.readall()\n\nimport asyncio\nasyncio.run(datalake_operations())\n```\n\n## Best Practices\n\n1. **Use hierarchical namespace** for file system semantics\n2. **Use `append_data` + `flush_data`** for large file uploads\n3. **Set ACLs at directory level** and inherit to children\n4. **Use async client** for high-throughput scenarios\n5. **Use `get_paths` with `recursive=True`** f",
  "tags": [
    "python",
    "api",
    "ai",
    "azure",
    "rag"
  ],
  "useCases": [],
  "scrapedAt": "2026-02-12T07:16:01.632Z"
}
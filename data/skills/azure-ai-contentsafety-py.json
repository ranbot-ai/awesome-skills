{
  "id": "antigravity-azure-ai-contentsafety-py",
  "name": "azure-ai-contentsafety-py",
  "slug": "azure-ai-contentsafety-py",
  "description": "Azure AI Content Safety SDK for Python. Use for detecting harmful content in text and images with multi-severity classification.\nTriggers: \"azure-ai-contentsafety\", \"ContentSafetyClient\", \"content moderation\", \"harmful content\", \"text analysis\", \"image analysis\".\n",
  "category": "AI & Agents",
  "source": "antigravity",
  "repoUrl": "https://github.com/sickn33/antigravity-awesome-skills",
  "skillUrl": "https://github.com/sickn33/antigravity-awesome-skills/tree/main/skills/azure-ai-contentsafety-py",
  "content": "\n# Azure AI Content Safety SDK for Python\n\nDetect harmful user-generated and AI-generated content in applications.\n\n## Installation\n\n```bash\npip install azure-ai-contentsafety\n```\n\n## Environment Variables\n\n```bash\nCONTENT_SAFETY_ENDPOINT=https://<resource>.cognitiveservices.azure.com\nCONTENT_SAFETY_KEY=<your-api-key>\n```\n\n## Authentication\n\n### API Key\n\n```python\nfrom azure.ai.contentsafety import ContentSafetyClient\nfrom azure.core.credentials import AzureKeyCredential\nimport os\n\nclient = ContentSafetyClient(\n    endpoint=os.environ[\"CONTENT_SAFETY_ENDPOINT\"],\n    credential=AzureKeyCredential(os.environ[\"CONTENT_SAFETY_KEY\"])\n)\n```\n\n### Entra ID\n\n```python\nfrom azure.ai.contentsafety import ContentSafetyClient\nfrom azure.identity import DefaultAzureCredential\n\nclient = ContentSafetyClient(\n    endpoint=os.environ[\"CONTENT_SAFETY_ENDPOINT\"],\n    credential=DefaultAzureCredential()\n)\n```\n\n## Analyze Text\n\n```python\nfrom azure.ai.contentsafety import ContentSafetyClient\nfrom azure.ai.contentsafety.models import AnalyzeTextOptions, TextCategory\nfrom azure.core.credentials import AzureKeyCredential\n\nclient = ContentSafetyClient(endpoint, AzureKeyCredential(key))\n\nrequest = AnalyzeTextOptions(text=\"Your text content to analyze\")\nresponse = client.analyze_text(request)\n\n# Check each category\nfor category in [TextCategory.HATE, TextCategory.SELF_HARM, \n                 TextCategory.SEXUAL, TextCategory.VIOLENCE]:\n    result = next((r for r in response.categories_analysis \n                   if r.category == category), None)\n    if result:\n        print(f\"{category}: severity {result.severity}\")\n```\n\n## Analyze Image\n\n```python\nfrom azure.ai.contentsafety import ContentSafetyClient\nfrom azure.ai.contentsafety.models import AnalyzeImageOptions, ImageData\nfrom azure.core.credentials import AzureKeyCredential\nimport base64\n\nclient = ContentSafetyClient(endpoint, AzureKeyCredential(key))\n\n# From file\nwith open(\"image.jpg\", \"rb\") as f:\n    image_data = base64.b64encode(f.read()).decode(\"utf-8\")\n\nrequest = AnalyzeImageOptions(\n    image=ImageData(content=image_data)\n)\n\nresponse = client.analyze_image(request)\n\nfor result in response.categories_analysis:\n    print(f\"{result.category}: severity {result.severity}\")\n```\n\n### Image from URL\n\n```python\nfrom azure.ai.contentsafety.models import AnalyzeImageOptions, ImageData\n\nrequest = AnalyzeImageOptions(\n    image=ImageData(blob_url=\"https://example.com/image.jpg\")\n)\n\nresponse = client.analyze_image(request)\n```\n\n## Text Blocklist Management\n\n### Create Blocklist\n\n```python\nfrom azure.ai.contentsafety import BlocklistClient\nfrom azure.ai.contentsafety.models import TextBlocklist\nfrom azure.core.credentials import AzureKeyCredential\n\nblocklist_client = BlocklistClient(endpoint, AzureKeyCredential(key))\n\nblocklist = TextBlocklist(\n    blocklist_name=\"my-blocklist\",\n    description=\"Custom terms to block\"\n)\n\nresult = blocklist_client.create_or_update_text_blocklist(\n    blocklist_name=\"my-blocklist\",\n    options=blocklist\n)\n```\n\n### Add Block Items\n\n```python\nfrom azure.ai.contentsafety.models import AddOrUpdateTextBlocklistItemsOptions, TextBlocklistItem\n\nitems = AddOrUpdateTextBlocklistItemsOptions(\n    blocklist_items=[\n        TextBlocklistItem(text=\"blocked-term-1\"),\n        TextBlocklistItem(text=\"blocked-term-2\")\n    ]\n)\n\nresult = blocklist_client.add_or_update_blocklist_items(\n    blocklist_name=\"my-blocklist\",\n    options=items\n)\n```\n\n### Analyze with Blocklist\n\n```python\nfrom azure.ai.contentsafety.models import AnalyzeTextOptions\n\nrequest = AnalyzeTextOptions(\n    text=\"Text containing blocked-term-1\",\n    blocklist_names=[\"my-blocklist\"],\n    halt_on_blocklist_hit=True\n)\n\nresponse = client.analyze_text(request)\n\nif response.blocklists_match:\n    for match in response.blocklists_match:\n        print(f\"Blocked: {match.blocklist_item_text}\")\n```\n\n## Severity Levels\n\nText analysis returns 4 severity levels (0, 2, 4, 6) by default. For 8 levels (0-7):\n\n```python\nfrom azure.ai.contentsafety.models import AnalyzeTextOptions, AnalyzeTextOutputType\n\nrequest = AnalyzeTextOptions(\n    text=\"Your text\",\n    output_type=AnalyzeTextOutputType.EIGHT_SEVERITY_LEVELS\n)\n```\n\n## Harm Categories\n\n| Category | Description |\n|----------|-------------|\n| `Hate` | Attacks based on identity (race, religion, gender, etc.) |\n| `Sexual` | Sexual content, relationships, anatomy |\n| `Violence` | Physical harm, weapons, injury |\n| `SelfHarm` | Self-injury, suicide, eating disorders |\n\n## Severity Scale\n\n| Level | Text Range | Image Range | Meaning |\n|-------|------------|-------------|---------|\n| 0 | Safe | Safe | No harmful content |\n| 2 | Low | Low | Mild references |\n| 4 | Medium | Medium | Moderate content |\n| 6 | High | High | Severe content |\n\n## Client Types\n\n| Client | Purpose |\n|--------|---------|\n| `ContentSafetyClient` | Analyze text and images |\n| `BlocklistClient` | Manage custom blocklists |\n\n## Best Practices\n\n1. **Use blocklists** for domain-specific terms\n2. **Set severity thr",
  "tags": [
    "python",
    "api",
    "ai",
    "image",
    "azure"
  ],
  "useCases": [],
  "scrapedAt": "2026-02-12T07:15:30.401Z"
}